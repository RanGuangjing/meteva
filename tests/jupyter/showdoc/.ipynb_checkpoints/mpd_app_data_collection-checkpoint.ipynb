{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import meteva.base as meb\n",
    "import meteva.product as mpd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检验工作的第一步是数据收集处理，micaps分布式数据库是一个非常稳定的数据来源，缺点是它只能提供实时业务数据，可提供的数据序列长度非常有限。检验工作的开展通常需要用到比较长的历史数据，数据序列越长越完整检验工作的开展则越顺利。如果无现成的序列完整又读取方便的数据，为保障检验工作的开展就需要自己收集整理数据。micaps分布式数据库因其稳定性好，且存储规范，读取方便等优点可以作为检验数据收集的稳定数据源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本模块是基于micaps分布式数据库构建的一套数据收集系统，用户根据需要从micaps上下载数据到本地，并保持相对路径的一致，便于后续开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据下载  \n",
    "**download_from_gds(para)**  \n",
    "根据参数从micaps服务器中下载数据  \n",
    "  \n",
    "  \n",
    "**参数说明：**  \n",
    " **para**: 数据下载配置参数，以下结合系统中集成的一个示例 mpd.application.data_collection.para_example进行阐述       \n",
    " **return**:   无          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_example= {\n",
    "    \"cup_count\":1,        # 并行下载的进程数\n",
    "    \"ip_port_file\":r\"H:\\test_data\\input\\input\\meb\\config.ini\",    #  micaps分布式服务器的ip和port配置文件\n",
    "    \"local_binary_dir\": r\"O:\\data\\mdfs\",  # 以二进制micaps格式保存网格数据的根目录\n",
    "    \"local_sta_dir\": r\"O:\\data\\sta\",      # 以二进制micaps格式保存站点数据的根目录\n",
    "    \"local_grid_dir\":r\"O:\\data\\grid\",     # 以netcdf格式保存网格数据的根目录\n",
    "    \"max_save_day\": 14,                   # 二进制micaps网格数据的最长保存日数\n",
    "    \"sta_origin_dirs\": [                                                               #待下载的站点数据列表\n",
    "        [\"mdfs:///SURFACE/QC_BY_FSOL/RAIN01_ALL_STATION/\", [[0, 2359]]],               #待下载的站点数据的路径和作业配置\n",
    "        [\"mdfs:///SURFACE/QC_BY_FSOL/WIND_AVERAGE_2MIN_ALL_STATION/\", [[0, 2359]]],\n",
    "        [\"mdfs:///SURFACE/QC_BY_FSOL/TMP_ALL_STATION/\", [[0, 2359]]],\n",
    "    ],\n",
    "    \"grid_origin_dirs\": {                                                          #待下载的网格数据字典\n",
    "        \"NWFD_SCMOC\": [\n",
    "            [\"NWFD_SCMOC/RAIN03\", [[1100, 1200],[2100,2300]]],                     #待下载的网格数据的路径和作业配置\n",
    "            [\"NWFD_SCMOC/TMP/2M_ABOVE_GROUND\", [[1100, 1200],[2100,2359]]],\n",
    "            [\"NWFD_SCMOC/WIND/10M_ABOVE_GROUND\", [[1100, 1200],[2100,2300]]],\n",
    "        ],\n",
    "        \"EWMWF\": [\n",
    "            [\"ECMWF_HR/TMP_2M/\", [[1100, 1200],[2200,2300]]],\n",
    "            [\"ECMWF_HR/APCP/\", [[1100, 1200],[2200,2300]]],\n",
    "            [\"mdfs:///ECMWF_HR/WIND_10M/\", [[1100, 1200],[2200,2300]]],\n",
    "        ],\n",
    "        \"GRAPES_GFS\": [\n",
    "            [\"mdfs:///GRAPES_GFS/WIND/10M_ABOVE_GROUND/\", [[1100, 1200],[2200,2300]]],\n",
    "            [\"GRAPES_GFS/TMP/2M_ABOVE_GROUND\", [[1100, 1200],[2200,2300]]],\n",
    "            [\"GRAPES_GFS/APCP\", [[1100, 1200],[2200,2300]]],\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参数说明：**    \n",
    "1、micaps分布式服务器的ip和port配置文件格式请参考[gds站点数据读取的说明](https://www.showdoc.cc/meteva?page_id=3975601856475911)  \n",
    "  \n",
    "2、二进制micaps格式网格数据是将micaps服务器上读取的文件字节流原封不动保存到文件中。该文件可以直接在micaps中显示。 同时由于其中它没有进行数据压缩，所以从该格式文件中能够直接指定网格点的值，在对效率要求极高的交互式检验中，需要用到该类格式的数据。\n",
    "  \n",
    "3、二进制micaps格式站点数据也是将micaps服务器上读取的文件字节流原封不动的保存到文件中。 该文件也可以直接在micaps中显示。     \n",
    "4、netcdf格式保存数据通过压缩，可以延长数据存储序列的长度  \n",
    "  \n",
    "5、文件下载后在根目录下相对存储目录会自动同分布式服务器中的相对目录保持一致。此外站点数据和nc格式网格数据被下载之后会自动根据文件名自动判断日期，并按日期对数据文件进行归档。  \n",
    "  \n",
    "6、由于二进制micaps格式网格数据比较消耗存储空间，因此通常不能存太长时间的序列，max_sava_day是用来控制序列长度的参数  \n",
    "  \n",
    "7、待下载的数据参数：不同的数据下载备份的频率和时段是不一样的，对于实况数据需要尽量实时下载，而预报数据最好选择micaps服务器空闲时段下载。下载参数是一个二层列表，以   [\"NWFD_SCMOC/RAIN03\", [[1100, 1200],[2100,2300]]],     为例，它代表download(para)函数只有在一天当中的11至12时、21-23时两个时段触发运行时才会从NWFD_SCMOC/RAIN03目录下载数据到本地。 基于该设计，调用download(para)执行下载的主程序在配置系统自动作业时可以简单地设置为逐小时或逐半小时执行，同时保证不同数据的下载时段做一定区分避免，作业拥堵。   \n",
    "\n",
    "   \n",
    "   \n",
    "   **调用示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "于2021-03-14 09:10:20.413233开始下载\n"
     ]
    }
   ],
   "source": [
    "mpd.download_from_gds(para_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保持本地数据序列长度  \n",
    "**remove(dir,save_day)**  \n",
    "如果磁盘空间有限，对于一些特别占据存储空间的数据，可能需要定期删除久远的历史数据。通过每天一次的定时作业调用该函数，根据将dir目录下距今超过save_day天的数据删除，可以保持本地的序列长度。   \n",
    "  \n",
    "  \n",
    "**参数说明：**  \n",
    " **dir**: 数据路径的根目录，下面可以有多个子目录  \n",
    " **save_day**: 数据保持的天数   \n",
    " **return**:   无   \n",
    "   \n",
    "**调用示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ee6e39b4e21b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara_example\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"local_binary_dir\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpara_example\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_save_day\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mh:\\task\\develop\\python\\git\\meteva\\meteva\\product\\application\\data_collection.py\u001b[0m in \u001b[0;36mremove\u001b[1;34m(dir, save_day)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mfile_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeteva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_tools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path_list_in_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\task\\develop\\python\\git\\meteva\\meteva\\base\\tool\\path_tools.py\u001b[0m in \u001b[0;36mget_path_list_in_dir\u001b[1;34m(root_dir, all_path)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mfi_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mget_path_list_in_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mall_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\task\\develop\\python\\git\\meteva\\meteva\\base\\tool\\path_tools.py\u001b[0m in \u001b[0;36mget_path_list_in_dir\u001b[1;34m(root_dir, all_path)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mfi_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mget_path_list_in_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mall_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\task\\develop\\python\\git\\meteva\\meteva\\base\\tool\\path_tools.py\u001b[0m in \u001b[0;36mget_path_list_in_dir\u001b[1;34m(root_dir, all_path)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mfi_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mget_path_list_in_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mall_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\task\\develop\\python\\git\\meteva\\meteva\\base\\tool\\path_tools.py\u001b[0m in \u001b[0;36mget_path_list_in_dir\u001b[1;34m(root_dir, all_path)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mfi_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m             \u001b[0mget_path_list_in_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi_d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mpd.remove(para_example[\"local_binary_dir\"],para_example[\"max_save_day\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   在基于预报检验层的功能函数开展检验时，通常需要将分散在逐个文件中的数据读取、拼接、匹配合并成一个DataFrame。将整合好的DataFrame输出到一个文件中，在后续开展中也可以反复使用，相比于重新读取逐个文件要便捷高效得多。基于meteva化模块化的编程思路，虽然在数据整个的过程中代码量也必将多，但框架结构是固定可复用的，用户通常只需把一些已有代码复制过来，修改一下文件路径、循环步长、读取函数等等语句即可完整数据收集模块的开发工作。同时这种模块化的特征使得它有被进一步封装的潜力，为此我们进一步将数据整合相关的模块进行了封装，形成了一个函数，在这个函数中只要设置合理的参数就可以一句话完整数据预处理工作。\n",
    "这样做有两个好处：\n",
    "   1. 原先收集数据需要编写代码，之后只需配置相应的参数即可。\n",
    "   2. 数据收集模块中并不需要每次都把所有数据取一遍再整合，只需读入最近更新的数据即可，这样可以提高运行效率。   \n",
    "   \n",
    "但也需要注意它的缺点：\n",
    "   1. 数据整合模块只能使用于一些常见数据的读取整合问题，有些特殊格式文件还是要另外编程读取\n",
    "   2. 为了扩展数据整合模块的适应能力，其配置参数的复杂性逐步加大，此时准确理解每个配置参数的含义也是会成为一项工作负担。基于这个原因，数据整合模块的不是万能的，未来也不会无限制的扩展该模块的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "**prepare_dataset(para)**  \n",
    "根据字典型的配置参数对数据进行预处理   \n",
    "  \n",
    "**参数说明：**  \n",
    " **para**: 数据下载配置参数，以下结合系统中集成的一个示例 mpd.application.data_prepare.para_example进行阐述       \n",
    " **return**:   无   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meteva\n",
    "import meteva.base as meb\n",
    "import meteva.product as mpd\n",
    "import datetime\n",
    "\n",
    "para_example= {\n",
    "    \"begin_time\":datetime.datetime.now() - datetime.timedelta(days =7),\n",
    "    \"end_time\":datetime.datetime.now(),\n",
    "    \"station_file\":r\"H:\\task\\other\\202009-veri_objective_method\\sta_info.m3\",\n",
    "    \"defalut_value\":0,\n",
    "    \"hdf_file_name\":\"last_week_data.h5\",\n",
    "    \"interp\": meteva.base.interp_gs_nearest,\n",
    "    \"ob_data\":{\n",
    "        \"hdf_dir\":r\"H:\\test_data\\output\\mpd\\application\\ob_rain24\",\n",
    "        \"dir_ob\": r\"Z:\\data\\surface\\jiany_rr\\r1\\YYMMDDHH.000\",\n",
    "        \"hour\":None,\n",
    "        \"read_method\": meteva.base.io.read_stadata_from_micaps3,\n",
    "        \"read_para\": {},\n",
    "        \"reasonable_value\": [0, 1000],\n",
    "        \"operation\":meteva.base.fun.sum_of_sta,\n",
    "        \"operation_para\": {\"used_coords\": [\"time\"], \"span\": 24},\n",
    "        \"time_type\": \"BT\",\n",
    "    },\n",
    "    \"fo_data\":{\n",
    "        \"ECMWF\": {\n",
    "            \"hdf_dir\": r\"H:\\test_data\\output\\mpd\\application\\rain24\",\n",
    "            \"dir_fo\": r\"O:\\data\\grid\\ECMWF_HR\\APCP\\YYYYMMDD\\YYMMDDHH.TTT.nc\",\n",
    "            \"hour\":[8,20,12],\n",
    "            \"dtime\":[0,240,12],\n",
    "            \"read_method\": meteva.base.io.read_griddata_from_nc,\n",
    "            \"read_para\": {},\n",
    "            \"operation\": meteva.base.fun.change,\n",
    "            \"operation_para\": {\"used_coords\": \"dtime\", \"delta\": 24},\n",
    "            \"time_type\": \"BT\",\n",
    "            \"move_fo_time\": 12\n",
    "        },\n",
    "\n",
    "        \"SCMOC\": {\n",
    "            \"hdf_dir\": r\"H:\\test_data\\output\\mpd\\application\\rain24\",\n",
    "            \"dir_fo\": r\"O:\\data\\grid\\NWFD_SCMOC\\RAIN03\\YYYYMMDD\\YYMMDDHH.TTT.nc\",\n",
    "            \"hour\": [8, 20,12],\n",
    "            \"dtime\":[3,240,3],\n",
    "            \"read_method\": meteva.base.io.read_griddata_from_nc,\n",
    "            \"read_para\": {},\n",
    "            \"operation\": meteva.base.fun.sum_of_sta,\n",
    "            \"operation_para\": {\"used_coords\": [\"dtime\"], \"span\": 24},\n",
    "            \"time_type\": \"BT\",\n",
    "            \"move_fo_time\": 0\n",
    "        },\n",
    "    },\n",
    "    \"output_dir\":r\"H:\\task\\other\\202009-veri_objective_method\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|para字典参数中各关键字|  |  |说明|    \n",
    "|:--|:--|:--|:--|      \n",
    "|**begin_time**| | |拼接后的数据保护的观测和预报数据的起始时间，在业务中如果是需要对一段固定起止时段的数据进行检验，则它应该设置为一个固定值，如果业务检验要准备的数据是滚动变化的，则它可根据datetime.datetime.now()函数来设置，如上例所示。|\n",
    "|**end_time**| | |拼接后的数据保护的观测和预报数据的结束时间。<font face=\"黑体\" color=red size =3>考虑到人们常常难以一次就能正确填写后面所有的参数，因此也需要应对的调试过程，建议先将end_time 设置成距离begin_time 不远的一个时间，先测试运行结果是否合理，如果合理再将end_time 设置为真实需求所需的结束时间</font>| \n",
    "|**station_file**| | |micaps第3类格式的站点表文件| \n",
    "|**defalut_value**| | |数据文件中不包含的站点将被设置为default_value| \n",
    "|**hdf_file_name**| | |拼接后的文件，以及匹配合并后的文件的文件名（不带文件夹部分）| \n",
    "|**interp**| | |网格数据到站点数据插值的方法| \n",
    "|**ob_data**| | |观测数据的参数设置| \n",
    "| | **hdf_dir**| |拼接后的观测数据存放目录| \n",
    "| | **dir_ob**| |拼接前观测数据文件路径的通配模型，具体可参考上述代码中的示例| \n",
    "| | **hour**| |观测数据文件中包含的hour_of_day 的集合，当设置为None时，程序会遍历0-23判断每个各小时的文件是否存在并读取，如果只有（只需）08时和20时的数据文件，既可以将该参数设置为None，也可以将该参数设置为[8,20]| \n",
    "| | **read_method**| |读取观测文件的函数方法，根据站点数据文件格式来选择具体的读取方法，可选的包括[站点数据读取](https://www.showdoc.com.cn/meteva?page_id=3975601856475911)模块中的各类函数。如果站点数据格式是采用hdf格式存储的sta_data，则该参数应该设置为pd.read_hdf。注意设置该参数时只需保留函数名称，不要加（）号| \n",
    "| | **read_para**| |上述read_method的参数中除了station,level,time,dtime,data_name,show 之外的其它参数，例如当数据文件时micaps1类数据，要从中读取温度要素，则除了设置\"read_method\":meb.read_stadata_from_micaps1_2_8之外，还需要设置column参数才能进一步明确调研函数的方式。此时，read_para 参数可设置为{\"column\":meb.m1_element_column.温度}, 当读取过程不需要带额外的参数时，则该参数设置为{}，如上面代码中的示例| \n",
    "| | **reasonable_value**| |从观测数据中读取的数据合理取值范围，超出范围的将会被删除|  \n",
    "| | **operation**| |如果读入的观测数据就是预报检验对标的数据，则该参数设置为None。 但有时读入的数据还不是预报对标的实况，举例1，读入的数据是逐小时的温度，但预报的是24小时变温，此时需要将温度实况转换为变温实况， 这时该参数就需要设置为 meb.change; 举例2， 读取的是逐小时的降水，但检验的是24小时的累计降水量，此时需要将1小时降水累加成24小时的降水量，此时该参数应该设置为meb.sum_of_sta|   \n",
    "| | **operation_para**| |如果读入的观测数据就是预报检验对标的数据，则该参数设置为{}。 但有时读入的数据还不是预报对标的实况，举例1，读入的数据是逐小时的温度，但预报的是24小时变温，此时需要将温度实况转换为变温实况， 这时operation参数就需要设置为 meb.change,而该参数这需要设置为{\"used_coords\": \"time\", \"delta\": 24}; 举例2， 读取的是逐小时的降水，但检验的是24小时的累计降水量，此时需要将1小时降水累加成24小时的降水量，此时operation参数应该设置为meb.sum_of_sta,该参数则应设置为{\"used_coords\": [\"time\"], \"span\": 24}|   \n",
    "| | **time_type**| | 观测数据的时间类型，北京时间为\"BT\"，世界时为\"UT\"|   \n",
    "|**fo_data**| | |预报数据的参数设置| \n",
    "|| **数据名称** | |预报数据的名称，它将出现在匹配合并后的sta_data的数据列名称中| \n",
    "| | | **hdf_dir** |拼接后的观测数据存放目录| \n",
    "| | | **dir_ob**|拼接前观测数据文件路径的通配模型，具体可参考上述代码中的示例| \n",
    "| | |**hour** |预报数据文件中包含起报时间的hour_of_day 的集合，当设置为None时，程序会遍历0-23判断每个各小时的文件是否存在并读取，如果只有（只需）08时和20时的数据文件，既可以将该参数设置为None，也可以将该参数设置为[8,20],后一种参数设置方法运行效率会高一些| \n",
    "| | |**read_method** |读取观测文件的函数方法，根据站点数据文件格式来选择具体的读取方法，可选的包括[站点数据读取](https://www.showdoc.com.cn/meteva?page_id=3975601856475911)模块中的各类函数，也可以是[网格数据读取](https://www.showdoc.com.cn/meteva?page_id=3975601833484385)模块中的各类函数。如果站点数据格式是采用hdf格式存储的sta_data，则该参数应该设置为pd.read_hdf。注意设置该参数时只需保留函数名称，不要加（）号| \n",
    "| | |**read_para** |上述read_method的参数中除了station,level,time,dtime,data_name,show 之外的其它参数| \n",
    "| | |**operation** |如果读入的观测预报数据就是观测检验对标的数据，则该参数设置为None。 但有时读入的数据还不是观测对标的物理量，举例1，读入的数据是逐小时的温度预报，但是检验的是24小时变温，此时需要将温度预报转换为变温预报， 这时该参数就需要设置为 meb.change; 举例2， 读取的是逐小时的降水预报，但检验的是24小时的降水量预报，此时需要将1小时降水预报累加成24小时的降水量预报，此时该参数应该设置为meb.sum_of_sta，举例3，读入的数据是随时效逐渐累加的降水量预报，检验对象是24小时降水量，则该参数应该设置为 meb.change|   \n",
    "| | | **operation_para** |如果读入的预报数据就是观测检验对标的数据，则该参数设置为{}。 但有时读入的预报数据还不是检验对象，在上述两个举例中，举例1，该参数应设置为{\"used_coords\": \"dtime\", \"delta\": 24}; 举例2， 该参数则应设置为{\"used_coords\": [\"dtime\"], \"span\": 24},举例3，该参数应该设置为{\"used_coords\":\"dtime\",\"delta\",24}|   \n",
    "| | |**time_type** | 观测数据的时间类型，北京时间为\"BT\"，世界时为\"UT\"|   \n",
    "| | |**move_fo_time** |是否平移预报的起报时间，例如为了将36小时时效的ECMWF模式预报和24小时时效的SCMOC网格预报对齐，则ECMWF模式对应的参数应该设置为12， 而SCMOC对应的参数应该设置为0|     \n",
    "|**output_dir** | | |  匹配合并后的数据的存放路径|   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.prepare_dataset(para_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
